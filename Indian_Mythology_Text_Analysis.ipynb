{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Sxook8Kx4hV6",
        "outputId": "5169111a-197c-44a3-8364-63ac33487286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/The Project Gutenberg eBook of Indi.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1307790888.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1307790888.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Split into paragraphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpara\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpara\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1307790888.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/The Project Gutenberg eBook of Indi.txt'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# File path\n",
        "DATASET_PATH = r\"/content/The Project Gutenberg eBook of Indi.txt\"\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "def load_dataset(path):\n",
        "    with open(path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "\n",
        "# Preprocess dataset\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "\n",
        "# Create vector space model\n",
        "def create_vector_space_model(corpus):\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "\n",
        "# Load pre-trained GPT2 model and tokenizer\n",
        "def load_gpt2_model():\n",
        "    model_name = \"gpt2\"\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "# Generate response with GPT-2\n",
        "def generate_response_gpt2(prompt, tokenizer, model, max_length=100):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "\n",
        "# Query handling\n",
        "def handle_query(query, vectorizer, tfidf_matrix, corpus, tokenizer, model, threshold=0.3):\n",
        "    query_vec = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "    best_match_idx = np.argmax(similarities)\n",
        "    best_match_score = similarities[best_match_idx]\n",
        "\n",
        "    if best_match_score >= threshold:\n",
        "        response = corpus[best_match_idx]\n",
        "    else:\n",
        "        response = f\"Answer for this question not present in the dataset. But this might be related: {generate_response_gpt2(query, tokenizer, model)}\"\n",
        "    return response\n",
        "\n",
        "\n",
        "# Evaluate chatbot\n",
        "def evaluate_chatbot(test_queries, vectorizer, tfidf_matrix, corpus, tokenizer, model):\n",
        "    responses = [handle_query(query, vectorizer, tfidf_matrix, corpus, tokenizer, model) for query in test_queries]\n",
        "    print(\"\\nChatbot Responses:\")\n",
        "    for i, response in enumerate(responses):\n",
        "        print(f\"Q{i + 1}: {test_queries[i]}\\nA{i + 1}: {response}\\n\")\n",
        "\n",
        "\n",
        "# Visualization\n",
        "def visualize_evaluation(similarities, labels):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.bar(range(len(similarities)), similarities, tick_label=labels)\n",
        "    plt.xlabel(\"Query\")\n",
        "    plt.ylabel(\"Cosine Similarity\")\n",
        "    plt.title(\"Similarity of Responses to Queries\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    print(\"Loading dataset...\")\n",
        "    dataset = load_dataset(DATASET_PATH)\n",
        "    corpus = dataset.split('\\n\\n')  # Split into paragraphs\n",
        "    corpus = [preprocess_text(para) for para in corpus if para.strip()]\n",
        "\n",
        "    print(\"Creating vector space model...\")\n",
        "    vectorizer, tfidf_matrix = create_vector_space_model(corpus)\n",
        "\n",
        "    print(\"Loading GPT-2 model...\")\n",
        "    tokenizer, model = load_gpt2_model()\n",
        "\n",
        "    print(\"Chatbot ready! Ask your questions.\")\n",
        "    while True:\n",
        "        query = input(\"You: \")\n",
        "        if query.lower() in ['exit', 'quit']:\n",
        "            print(\"Exiting chatbot. Goodbye!\")\n",
        "            break\n",
        "        response = handle_query(query, vectorizer, tfidf_matrix, corpus, tokenizer, model)\n",
        "        print(f\"Bot: {response}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Preprocess Dataset\n",
        "def preprocess_and_split_dataset(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Remove Project Gutenberg metadata\n",
        "    text = re.split(r'\\*\\*\\* START OF.*?\\*\\*\\*', text, maxsplit=1)[-1]\n",
        "    text = re.split(r'\\*\\*\\* END OF.*?\\*\\*\\*', text, maxsplit=1)[0]\n",
        "\n",
        "    # Split into chunks (e.g., paragraphs)\n",
        "    paragraphs = [p.strip() for p in re.split(r'\\n\\n+', text) if p.strip()]\n",
        "    return paragraphs\n",
        "\n",
        "\n",
        "# Step 2: Retrieve Relevant Chunks Using Keyword Search\n",
        "def retrieve_relevant_chunk(question, paragraphs, top_k=3):\n",
        "    question_words = set(question.lower().split())\n",
        "    scored_paragraphs = []\n",
        "\n",
        "    for para in paragraphs:\n",
        "        para_words = set(para.lower().split())\n",
        "        common_words = question_words.intersection(para_words)\n",
        "        score = len(common_words)  # Count common words as relevance score\n",
        "        scored_paragraphs.append((para, score))\n",
        "\n",
        "    # Sort paragraphs by score (descending) and select top_k\n",
        "    scored_paragraphs = sorted(scored_paragraphs, key=lambda x: x[1], reverse=True)\n",
        "    relevant_chunks = [para for para, score in scored_paragraphs[:top_k]]\n",
        "    return relevant_chunks\n",
        "\n",
        "\n",
        "# Step 3: Load GPT Model for Text Generation\n",
        "def load_gpt_model():\n",
        "    print(\"Loading GPT model...\")\n",
        "    model_name = \"gpt2\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "# Step 4: Generate Response Using GPT\n",
        "def generate_response_gpt(question, context, model, tokenizer):\n",
        "    # Limit the context length and truncate if necessary\n",
        "    context = \" \".join(context.split()[:500])  # Limit context to a maximum of 500 tokens if necessary\n",
        "\n",
        "    prompt = (\n",
        "        f\"You are a knowledgeable assistant. Use the context below to answer the question.\\n\\n\"\n",
        "        f\"Context: {context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    # Encode the prompt with truncation\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "\n",
        "    # Check if input length exceeds model's max length and truncate if necessary\n",
        "    if input_ids.shape[1] > 1024:\n",
        "        print(\"Input is too long, truncating context.\")\n",
        "        input_ids = input_ids[:, :1024]  # Truncate the input to fit the model's max length\n",
        "\n",
        "    # Generate the response\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=200,  # Number of tokens to generate\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    # Decode the generated output\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "\n",
        "# Step 5: Evaluate Responses\n",
        "def evaluate_responses(questions, contexts, generated_responses):\n",
        "    bleu_scores = []\n",
        "    smoothing_function = SmoothingFunction().method1\n",
        "\n",
        "    for i, (context, response) in enumerate(zip(contexts, generated_responses)):\n",
        "        # BLEU score with smoothing\n",
        "        bleu_score = sentence_bleu([context.split()], response.split(), smoothing_function=smoothing_function)\n",
        "        bleu_scores.append(bleu_score)\n",
        "\n",
        "    return bleu_scores\n",
        "\n",
        "\n",
        "# Step 6: Visualize Metrics\n",
        "def visualize_metrics(bleu_scores):\n",
        "    x = range(len(bleu_scores))\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(x, bleu_scores, label='BLEU Score', marker='o')\n",
        "    plt.title('Evaluation Metrics')\n",
        "    plt.xlabel('Question Index')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    dataset_path = r\"/content/The Project Gutenberg eBook of Indi.txt\"\n",
        "\n",
        "    print(\"Preprocessing and splitting dataset...\")\n",
        "    paragraphs = preprocess_and_split_dataset(dataset_path)\n",
        "    gpt_model, gpt_tokenizer = load_gpt_model()\n",
        "\n",
        "    print(\"\\nChatbot is ready! Ask your questions below. Type 'exit' to finish and calculate metrics.\")\n",
        "\n",
        "    # Interactive loop to collect questions and generate answers\n",
        "    questions = []\n",
        "    contexts = []\n",
        "    generated_responses = []\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nYour Question: \")\n",
        "        if question.lower() in ['exit', 'quit']:\n",
        "            break\n",
        "\n",
        "        # Retrieve relevant chunks and generate answer\n",
        "        relevant_chunks = retrieve_relevant_chunk(question, paragraphs)\n",
        "        context = \" \".join(relevant_chunks[:3])  # Use top 3 chunks for context\n",
        "        response = generate_response_gpt(question, context, gpt_model, gpt_tokenizer)\n",
        "\n",
        "        print(f\"\\nAnswer: {response}\")\n",
        "        questions.append(question)\n",
        "        contexts.append(context)\n",
        "        generated_responses.append(response)\n",
        "\n",
        "    # Evaluate collected responses\n",
        "    print(\"\\nEvaluating responses...\")\n",
        "    bleu_scores = evaluate_responses(questions, contexts, generated_responses)\n",
        "\n",
        "    # Visualize metrics\n",
        "    print(\"\\nVisualizing metrics...\")\n",
        "    visualize_metrics(bleu_scores)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "vta3vMwz4o4T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}